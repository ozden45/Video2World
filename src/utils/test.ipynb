{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86479645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "#   sfm_point_generator.py\n",
    "#\n",
    "#   Description:\n",
    "#       This script generates 3D points from video data using \n",
    "#       Structure from Motion (SfM).\n",
    "#   \n",
    "#   Author: Özden Özel\n",
    "#   Created: 2026-01-28\n",
    "#\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2 as cv\n",
    "from matplotlib import colors\n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generate_sfm_points(video_path: str, cam_traj_path: str,  output_path: str):\n",
    "    \"\"\"\n",
    "    Generates 3D points from video data using Structure from Motion (SfM).\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        cam_traj_path (str): Path to the camera trajectory file.\n",
    "        output_path (str): Path to save the generated 3D points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check is the video file exists\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    # Check if the camera trajectory file exists\n",
    "    if not os.path.exists(cam_traj_path):\n",
    "        raise FileNotFoundError(f\"Camera trajectory file not found: {cam_traj_path}\")\n",
    "\n",
    "    # Read the video file\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    \n",
    "    \n",
    "\n",
    "def frame_to_sfm_points(frame: torch.Tensor, depth: torch.Tensor, R: torch.Tensor, t: torch.Tensor, cam_specs: dict) -> Tuple[List[int], List[torch.Tensor], List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Converts a video frame to 3D points using SfM techniques.\n",
    "\n",
    "    Args:\n",
    "        frame (cv.Mat): Input video frame.\n",
    "        depth (cv.Mat): Depth map corresponding to the frame.\n",
    "        R (cv.Mat): Rotation matrix of the camera.\n",
    "        t (cv.Mat): Translation matrix of the camera.\n",
    "        cam_specs (dict): Camera specifications including intrinsic parameters.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, torch.Tensor, torch.Tensor]]: The frame index and its generated 3D points in the format [μ, color].\n",
    "    \"\"\"\n",
    "\n",
    "    focal_length = cam_specs['focal_length']\n",
    "    cx = cam_specs['cx']\n",
    "    cy = cam_specs['cy']\n",
    "\n",
    "    \"\"\"\n",
    "    x = f * Xc / Zc --> Xc = x * Zc / f\n",
    "    y = f * Yc / Zc --> Yc = y * Zc / f\n",
    "\n",
    "    [Xc, Yc, Zc]^T = R * [Xw, Yw, Zw]^T + t --> [Xw, Yw, Zw]^T = R^-1 * ([Xc, Yc, Zc]^T - t)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    frame_indices = []\n",
    "    world_coords = []\n",
    "    colors = []\n",
    "    for y in range(frame.shape[0]-800):\n",
    "        for x in range(frame.shape[1]-800):\n",
    "            if y % 100 == 0 and x % 100 == 0:\n",
    "                print(f\"Processing pixel ({x}, {y})\")\n",
    "\n",
    "            Zc = depth[y, x]\n",
    "            if Zc == 0:\n",
    "                print(\"Invalid depth at pixel ({x}, {y}), skipping.\")\n",
    "                continue  # Skip invalid depth\n",
    "            Xc = (x - frame.shape[1] * .5) * Zc / focal_length\n",
    "            Yc = (y - frame.shape[0] * .5) * Zc / focal_length\n",
    "            # Form the camera coordinates\n",
    "            cam_coords = torch.tensor([[Xc], [Yc], [Zc]])\n",
    "            color = frame[y, x]\n",
    "            # Convert to world coordinates\n",
    "            R_inv = torch.inverse(torch.tensor(R))\n",
    "            t_tensor = torch.tensor(t).reshape(3, 1)\n",
    "            \n",
    "            frame_indices.append(1)\n",
    "            world_coords.append(R_inv @ (torch.transpose(cam_coords, 0, 1) - t_tensor))\n",
    "            colors.append(color)\n",
    "            \n",
    "\n",
    "    return frame_indices, world_coords, colors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def converge_sfm_points(points_list: List[torch.Tensor], delta: List[Tuple[float, float, float]]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converges multiple sets of 3D points into a single coherent set.\n",
    "    Args:\n",
    "        points_list (List[torch.Tensor]): List of 3D point tensors.\n",
    "    Returns:\n",
    "        torch.Tensor: Converged 3D points.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    script_dir = os.path.dirname(__file__)\n",
    "    frame_path = os.path.join(script_dir, 'dog.jpg')\n",
    "    depth_path = os.path.join(script_dir, 'depth.npy')\n",
    "\n",
    "    frame = cv.imread(frame_path)  # returns None if file unreadable\n",
    "    if frame is None:\n",
    "        raise FileNotFoundError(f\"Image not found or unreadable: {frame_path}\")\n",
    "    frame = torch.Tensor(frame)\n",
    "\n",
    "    if not os.path.exists(depth_path):\n",
    "        raise FileNotFoundError(f\"Depth file not found: {depth_path}\")\n",
    "    depth = np.load(depth_path)\n",
    "    depth = torch.Tensor(depth)\n",
    "\n",
    "    print(f\"Frame shape: {frame.shape}, Depth shape: {depth.shape}\")\n",
    "    print(f\"Frame type: {type(frame)}, Depth type: {type(depth)}\")\n",
    "\n",
    "    R = torch.Tensor([\n",
    "        [1, 2, 4], \n",
    "        [0, 4, 2], \n",
    "        [0, 0, 2]\n",
    "        ])\n",
    "    t = torch.Tensor([1, 2, 1])\n",
    "\n",
    "    cam_specs = {\n",
    "        'focal_length': 50e-3,\n",
    "        'cx': 320,\n",
    "        'cy': 240\n",
    "    }\n",
    "    \n",
    "    frame_indices, world_coords, colors = frame_to_sfm_points(frame, depth, R, t, cam_specs)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "    print(f\"x_shape: {world_coords[:][0].shape}, y_shape: {world_coords[:][1].shape}, z_shape: {world_coords[:][2].shape}\")\n",
    "    ax.scatter(world_coords[:][0], world_coords[:][1], world_coords[:][2], s=2)\n",
    "\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12ce210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame shape: torch.Size([1213, 1546, 3]), Depth shape: torch.Size([1213, 1546])\n",
      "Frame type: <class 'torch.Tensor'>, Depth type: <class 'torch.Tensor'>\n",
      "\n",
      "R: tensor([[1., 2., 4.],\n",
      "        [0., 4., 2.],\n",
      "        [0., 0., 2.]]), R_shape: torch.Size([3, 3])\n",
      "t: tensor([1., 2., 1.]), t_shape: torch.Size([3])\n",
      "\n",
      "================================================================\n",
      "================================================================\n",
      "\n",
      "cam_coords: tensor([[-3.8006e+04],\n",
      "        [-2.8604e+04],\n",
      "        [ 2.8237e+00]]), cam_Coords_shape: torch.Size([3, 1])\n",
      "\n",
      "color: tensor([ 5., 11., 10.]), color_shape: torch.Size([3])\n",
      "\n",
      "R_inv: tensor([[ 1.0000, -0.5000, -1.5000],\n",
      "        [ 0.0000,  0.2500, -0.2500],\n",
      "        [ 0.0000,  0.0000,  0.5000]]), R_inv_shape: torch.Size([3, 3])\n",
      "\n",
      "t_tensor: tensor([[1.],\n",
      "        [2.],\n",
      "        [1.]]), t_tensor_shape: torch.Size([3, 1])\n",
      "\n",
      "torch.transpose(cam_coords, 0, 1): tensor([[-3.8006e+04, -2.8604e+04,  2.8237e+00]]), shape: torch.Size([1, 3])\n",
      "\n",
      "world_coords: tensor([[-2.3707e+04],\n",
      "        [-7.1519e+03],\n",
      "        [ 9.1183e-01]]), world_coords_shape: torch.Size([3, 1])\n",
      "\n",
      "colors: tensor([ 5., 11., 10.]), colors_shape: torch.Size([3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "from matplotlib import colors\n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "script_dir = \"C:\\\\Users\\\\ozden\\\\source\\\\repos\\\\Video2World\\\\src\\\\utils\"\n",
    "frame_path = os.path.join(script_dir, 'dog.jpg')\n",
    "depth_path = os.path.join(script_dir, 'depth.npy')\n",
    "\n",
    "frame = cv.imread(frame_path)  # returns None if file unreadable\n",
    "if frame is None:\n",
    "    raise FileNotFoundError(f\"Image not found or unreadable: {frame_path}\")\n",
    "frame = torch.Tensor(frame)\n",
    "\n",
    "if not os.path.exists(depth_path):\n",
    "    raise FileNotFoundError(f\"Depth file not found: {depth_path}\")\n",
    "depth = np.load(depth_path)\n",
    "depth = torch.Tensor(depth)\n",
    "\n",
    "print(f\"Frame shape: {frame.shape}, Depth shape: {depth.shape}\")\n",
    "print(f\"Frame type: {type(frame)}, Depth type: {type(depth)}\\n\")\n",
    "\n",
    "R = torch.Tensor([\n",
    "    [1, 2, 4], \n",
    "    [0, 4, 2], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "t = torch.Tensor([1, 2, 1])\n",
    "\n",
    "cam_specs = {\n",
    "    'focal_length': 50e-3,\n",
    "    'cx': 320,\n",
    "    'cy': 240\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"R: {R}, R_shape: {R.shape}\")\n",
    "print(f\"t: {t}, t_shape: {t.shape}\")\n",
    "\n",
    "print(\"\\n================================================================\")\n",
    "print(\"================================================================\\n\")\n",
    "\n",
    "# ========================================================================\n",
    "# ========================================================================\n",
    "\n",
    "y = 100\n",
    "x = 100\n",
    "focal_length = cam_specs['focal_length']\n",
    "\n",
    "Zc = depth[y, x]\n",
    "if Zc == 0:\n",
    "    print(f\"Invalid depth at pixel ({x}, {y}), skipping.\")\n",
    "else: \n",
    "    Xc = (x - frame.shape[1] * .5) * Zc / focal_length\n",
    "    Yc = (y - frame.shape[0] * .5) * Zc / focal_length\n",
    "    \n",
    "    # Form the camera coordinates\n",
    "    cam_coords = torch.tensor([[Xc], [Yc], [Zc]])\n",
    "    color = frame[y, x]\n",
    "    \n",
    "    print(f\"cam_coords: {cam_coords}, cam_Coords_shape: {cam_coords.shape}\\n\")\n",
    "    print(f\"color: {color}, color_shape: {color.shape}\\n\")\n",
    "    \n",
    "    # Convert to world coordinates\n",
    "    R_inv = torch.inverse(R)\n",
    "    t_tensor = t.reshape(3, 1)\n",
    "\n",
    "    print(f\"R_inv: {R_inv}, R_inv_shape: {R_inv.shape}\\n\")\n",
    "    print(f\"t_tensor: {t_tensor}, t_tensor_shape: {t_tensor.shape}\\n\")\n",
    "    print(f\"torch.transpose(cam_coords, 0, 1): {torch.transpose(cam_coords, 0, 1)}, shape: {torch.transpose(cam_coords, 0, 1).shape}\\n\")\n",
    "\n",
    "    world_coords = R_inv @ (cam_coords - t_tensor)\n",
    "    colors = color\n",
    "    \n",
    "    print(f\"world_coords: {world_coords}, world_coords_shape: {world_coords.shape}\\n\")\n",
    "    print(f\"colors: {colors}, colors_shape: {colors.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57bccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame shape: torch.Size([1213, 1546, 3]), Depth shape: torch.Size([1213, 1546])\n",
      "Frame type: <class 'torch.Tensor'>, Depth type: <class 'torch.Tensor'>\n",
      "\n",
      "R: tensor([[1., 2., 4.],\n",
      "        [0., 4., 2.],\n",
      "        [0., 0., 2.]]), R_shape: torch.Size([3, 3])\n",
      "\n",
      "t: tensor([1., 2., 1.]), t_shape: torch.Size([3])\n",
      "\n",
      "================================================================\n",
      "================================================================\n",
      "\n",
      "Processing pixel (0, 0)\n",
      "Processing pixel (100, 0)\n",
      "Processing pixel (200, 0)\n",
      "Processing pixel (300, 0)\n",
      "Processing pixel (400, 0)\n",
      "Processing pixel (500, 0)\n",
      "Processing pixel (600, 0)\n",
      "Processing pixel (700, 0)\n",
      "Processing pixel (0, 100)\n",
      "Processing pixel (100, 100)\n",
      "Processing pixel (200, 100)\n",
      "Processing pixel (300, 100)\n",
      "Processing pixel (400, 100)\n",
      "Processing pixel (500, 100)\n",
      "Processing pixel (600, 100)\n",
      "Processing pixel (700, 100)\n",
      "Processing pixel (0, 200)\n",
      "Processing pixel (100, 200)\n",
      "Processing pixel (200, 200)\n",
      "Processing pixel (300, 200)\n",
      "Processing pixel (400, 200)\n",
      "Processing pixel (500, 200)\n",
      "Processing pixel (600, 200)\n",
      "Processing pixel (700, 200)\n",
      "Processing pixel (0, 300)\n",
      "Processing pixel (100, 300)\n",
      "Processing pixel (200, 300)\n",
      "Processing pixel (300, 300)\n",
      "Processing pixel (400, 300)\n",
      "Processing pixel (500, 300)\n",
      "Processing pixel (600, 300)\n",
      "Processing pixel (700, 300)\n",
      "Processing pixel (0, 400)\n",
      "Processing pixel (100, 400)\n",
      "Processing pixel (200, 400)\n",
      "Processing pixel (300, 400)\n",
      "Processing pixel (400, 400)\n",
      "Processing pixel (500, 400)\n",
      "Processing pixel (600, 400)\n",
      "Processing pixel (700, 400)\n",
      "world_coords.shape: 308098\n",
      "colors.shape: 308098\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "script_dir = \"C:\\\\Users\\\\ozden\\\\source\\\\repos\\\\Video2World\\\\src\\\\utils\"\n",
    "frame_path = os.path.join(script_dir, 'dog.jpg')\n",
    "depth_path = os.path.join(script_dir, 'depth.npy')\n",
    "\n",
    "frame = cv.imread(frame_path)  # returns None if file unreadable\n",
    "if frame is None:\n",
    "    raise FileNotFoundError(f\"Image not found or unreadable: {frame_path}\")\n",
    "frame = torch.Tensor(frame)\n",
    "\n",
    "if not os.path.exists(depth_path):\n",
    "    raise FileNotFoundError(f\"Depth file not found: {depth_path}\")\n",
    "depth = np.load(depth_path)\n",
    "depth = torch.Tensor(depth)\n",
    "\n",
    "print(f\"Frame shape: {frame.shape}, Depth shape: {depth.shape}\")\n",
    "print(f\"Frame type: {type(frame)}, Depth type: {type(depth)}\\n\")\n",
    "\n",
    "R = torch.Tensor([\n",
    "    [1, 2, 4], \n",
    "    [0, 4, 2], \n",
    "    [0, 0, 2]\n",
    "    ])\n",
    "t = torch.Tensor([1, 2, 1])\n",
    "\n",
    "cam_specs = {\n",
    "    'focal_length': 50e-3,\n",
    "    'cx': 320,\n",
    "    'cy': 240\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"R: {R}, R_shape: {R.shape}\\n\")\n",
    "print(f\"t: {t}, t_shape: {t.shape}\")\n",
    "\n",
    "print(\"\\n================================================================\")\n",
    "print(\"================================================================\\n\")\n",
    "\n",
    "# ========================================================================\n",
    "# ========================================================================\n",
    "\n",
    "y = 100\n",
    "x = 100\n",
    "focal_length = cam_specs['focal_length']\n",
    "\n",
    "world_coords = []\n",
    "colors = []\n",
    "\n",
    "for y in range(frame.shape[0]-800):\n",
    "    for x in range(frame.shape[1]-800):\n",
    "        if y % 100 == 0 and x % 100 == 0:\n",
    "            print(f\"Processing pixel ({x}, {y})\")\n",
    "\n",
    "        Zc = depth[y, x]\n",
    "        if Zc == 0:\n",
    "            print(f\"Invalid depth at pixel ({x}, {y}), skipping.\")\n",
    "        else: \n",
    "            Xc = (x - frame.shape[1] * .5) * Zc / focal_length\n",
    "            Yc = (y - frame.shape[0] * .5) * Zc / focal_length\n",
    "\n",
    "            # Form the camera coordinates\n",
    "            cam_coords = torch.tensor([[Xc], [Yc], [Zc]])\n",
    "            color = frame[y, x]\n",
    "\n",
    "            #print(f\"cam_coords: {cam_coords}, cam_Coords_shape: {cam_coords.shape}\\n\")\n",
    "            #print(f\"color: {color}, color_shape: {color.shape}\\n\")\n",
    "\n",
    "            # Convert to world coordinates\n",
    "            R_inv = torch.inverse(R)\n",
    "            t_tensor = t.reshape(3, 1)\n",
    "\n",
    "            #print(f\"R_inv: {R_inv}, R_inv_shape: {R_inv.shape}\\n\")\n",
    "            #print(f\"t_tensor: {t_tensor}, t_tensor_shape: {t_tensor.shape}\\n\")\n",
    "            #print(f\"torch.transpose(cam_coords, 0, 1): {torch.transpose(cam_coords, 0, 1)}, shape: {torch.transpose(cam_coords, 0, 1).shape}\")\n",
    "\n",
    "            world_coords.append(R_inv @ (cam_coords - t_tensor))\n",
    "            colors.append(color)\n",
    "\n",
    "print(f\"world_coords.shape: {len(world_coords)}\")\n",
    "print(f\"colors.shape: {len(colors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc0a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world_coords.len: 308098, world_coords[0].shape: torch.Size([3, 1])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_coords.len: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(world_coords)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, world_coords[0].shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld_coords[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolors.len: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(colors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, colors[0].shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "depth.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc146e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point 1: SFMPoint(coords=tensor([1., 2., 3.]), covariance=tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]]), color=tensor([255,   0,   0]), alpha=tensor(1.))\n",
      "Point 2: SFMPoint(coords=tensor([1., 2., 3.]), covariance=tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]]), color=tensor([255,   0,   0]), alpha=tensor(1.))\n",
      "Point 1: SFMPoint(coords=tensor([1., 2., 3.]), covariance=tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]]), color=tensor([255,   0,   0]), alpha=tensor(1.))\n",
      "Are points equal? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SFMPoint:\n",
    "    coords: torch.Tensor       # 3D coordinates\n",
    "    covariance: torch.Tensor   # Covariance matrix\n",
    "    color: torch.Tensor        # Color information\n",
    "    alpha: torch.Tensor        # Opacity\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, SFMPoint):\n",
    "            return False\n",
    "        return (torch.equal(self.coords, other.coords) and\n",
    "                torch.equal(self.covariance, other.covariance) and\n",
    "                torch.equal(self.color, other.color) and\n",
    "                torch.equal(self.alpha, other.alpha))\n",
    "\n",
    "\n",
    "\n",
    "p1 = SFMPoint(\n",
    "    coords = torch.tensor([1.0, 3.0, 3.0]),\n",
    "    covariance = torch.eye(3),\n",
    "    color = torch.tensor([255, 0, 0]),\n",
    "    alpha = torch.tensor(1.0) \n",
    ")\n",
    "\n",
    "p2 = SFMPoint(\n",
    "    coords = torch.tensor([1.0, 2.0, 3.0]),\n",
    "    covariance = torch.eye(3),\n",
    "    color = torch.tensor([255, 0, 0]),\n",
    "    alpha = torch.tensor(1.0)\n",
    ")\n",
    "print(f\"Point 1: {p1}\")\n",
    "print(f\"Point 2: {p2}\")\n",
    "print(f\"Point 1: {p1}\")\n",
    "print(f\"Are points equal? {p1 == p2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
